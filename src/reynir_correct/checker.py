"""

    Greynir: Natural language processing for Icelandic

    Spelling and grammar checking module

    Copyright © 2025 Miðeind ehf.

    This software is licensed under the MIT License:

        Permission is hereby granted, free of charge, to any person
        obtaining a copy of this software and associated documentation
        files (the "Software"), to deal in the Software without restriction,
        including without limitation the rights to use, copy, modify, merge,
        publish, distribute, sublicense, and/or sell copies of the Software,
        and to permit persons to whom the Software is furnished to do so,
        subject to the following conditions:

        The above copyright notice and this permission notice shall be
        included in all copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
        EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
        MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
        IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
        CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
        TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
        SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


    This module exposes functions to check spelling and grammar for
    text strings.

    It defines subclasses of the classes BIN_Token and Fast_Parser,
    both found in the Greynir package. These classes add error detection
    functionality to their base classes. After parsing a sentence, the
    ErrorFinder and PatternMatcher classes are used to identify grammar
    errors and questionable patterns.

    Error codes generated by this module:
    -------------------------------------

    E001: The sentence could not be parsed.
    E002: A nonterminal tagged with 'error' is present in the parse tree.
    E003: An impersonal verb occurs with an incorrect subject case.
    E004: The sentence is probably not in Icelandic.
    E005: The sentence is probably too long.
    E006: Abbreviations that should be written in full in formal texts.
    E007: Sentence contains exclamation marks, inappropriate in formal texts.

"""

from typing import Any, Dict, Iterable, Iterator, List, Mapping, Optional, Tuple, cast
from typing_extensions import TypedDict

import os
import importlib.util
from importlib.abc import Loader
from importlib.machinery import ModuleSpec
from threading import Lock
from types import ModuleType

from islenska.basics import Ksnid
from reynir import TOK, Greynir, Paragraph, Sentence, TokenList, _Job, correct_spaces
from reynir.binparser import BIN_Grammar, BIN_Parser, VariantHandler
from reynir.bintokenizer import StringIterable
from reynir.fastparser import ffi  # type: ignore
from reynir.fastparser import Fast_Parser
from reynir.incparser import ICELANDIC_RATIO
from reynir.reducer import Reducer
from reynir.reynir import Job, ProgressFunc, DEFAULT_MAX_SENT_TOKENS
from tokenizer import Abbreviations, Tok

from .settings import Settings
from .annotation import Annotation
from .errfinder import ErrorDetectionToken, ErrorFinder
from .errtokenizer import CorrectionPipeline, CorrectToken, settings_or_default
from .pattern import PatternMatcher

# Style mark from BÍN:
# NID = Niðrandi / disparaging
# OVID = Óviðeigandi / inappropriate
# URE = Úrelt / obsolete
# SJALD = Sjaldgæft / rare
# VILLA = Villa / error
# GAM = Gamalt / old
STYLE_WARNINGS: Mapping[str, str] = {
    "NID": "niðrandi",
    "OVID": "óviðeigandi",
    "URE": "úrelt",
    "SJALD": "sjaldgæft",
    "VILLA": "villa",
    "GAM": "gamalt",
}


def style_warning(k: Ksnid) -> str:
    """Return a style warning for the given Ksnid tuple, if any"""
    if k.malsnid in STYLE_WARNINGS:
        return k.malsnid
    if k.bmalsnid in STYLE_WARNINGS:
        return k.bmalsnid
    return ""


class ErrorDetectingGrammar(BIN_Grammar):
    """A subclass of BIN_Grammar that causes conditional sections in the
    Greynir.grammar file, demarcated using
    $if(include_errors)...$endif(include_errors),
    to be included in the grammar as it is read and parsed"""

    def __init__(self) -> None:
        super().__init__()
        # Enable the 'include_errors' condition
        self.set_conditions({"include_errors"})


class AnnotatedSentence(Sentence):
    """A subclass that adds a list of Annotation instances to a Sentence object"""

    def __init__(self, job: Job, s: TokenList) -> None:
        super().__init__(job, s)
        self.annotations: List[Annotation] = []
        # Convert the list of tokens to a list of CorrectToken instances, avoids many casts
        self.correct_tokens: List[CorrectToken] = cast(List[CorrectToken], self.tokens)


# The type of a grammar check result
class CheckResult(TypedDict):
    sentences: List[AnnotatedSentence]
    num_sentences: int
    num_parsed: int
    num_tokens: int
    ambiguity: float
    parse_time: float


class ErrorDetectingParser(Fast_Parser):

    """A subclass of Fast_Parser that modifies its behavior to
    include grammar error detection rules in the parsing process"""

    _GRAMMAR_BINARY_FILE = Fast_Parser._GRAMMAR_FILE + ".error.bin"

    # Keep a separate grammar class instance and time stamp for
    # ErrorDetectingParser. This Python sleight-of-hand overrides
    # class attributes that are defined in BIN_Parser, see binparser.py.
    _grammar_ts: Optional[float] = None
    _grammar: Optional[BIN_Grammar] = None
    _grammar_class = ErrorDetectingGrammar

    # Also keep separate class instances of the C grammar and its timestamp
    _c_grammar: Any = cast(Any, ffi).NULL
    _c_grammar_ts: Optional[float] = None

    @staticmethod
    def wrap_token(t: Tok, ix: int) -> ErrorDetectionToken:
        """Create an instance of a wrapped token"""
        return ErrorDetectionToken(t, ix)


class GreynirCorrect(Greynir):
    """Parser augmented with the ability to add spelling and grammar
    annotations to the returned sentences"""

    # GreynirCorrect has its own class instances of a parser and a reducer,
    # separate from the Greynir class, as they use different settings and
    # parsing enviroments
    _parser: Optional[ErrorDetectingParser] = None
    _reducer = None
    _lock = Lock()

    def __init__(
        self,
        settings: Settings,
        pipeline: CorrectionPipeline,
        **options: Any,
    ) -> None:
        super().__init__(**options)
        self.settings = settings
        self.pipeline = pipeline

    def tokenize(self, text: StringIterable) -> Iterator[Tok]:
        """Use the correcting tokenizer instead of the normal one"""
        # This is a bit of a hack: we set the pipeline's text_or_gen
        self.pipeline._text_or_gen = text  # type: ignore[reportPrivateUsage]
        return self.pipeline.tokenize()

    @classmethod
    def _dump_token(cls, tok: Tok) -> Tuple[Any, ...]:
        """Override token dumping function from Greynir,
        providing a JSON-dumpable object"""
        assert isinstance(tok, CorrectToken)
        return CorrectToken.dump(tok)

    @classmethod
    def _load_token(cls, *args: Any) -> CorrectToken:
        """Load token from serialized data"""
        largs = len(args)
        if largs == 3:
            # Plain ol' token
            return cast(CorrectToken, super()._load_token(*args))
        # This is a CorrectToken: pass it to that class for handling
        return CorrectToken.load(*args)

    @property
    def parser(self) -> Fast_Parser:
        """Override the parent class' construction of a parser instance"""
        with self._lock:
            if GreynirCorrect._parser is None or GreynirCorrect._parser.is_grammar_modified()[0]:
                # Initialize a singleton instance of the parser and the reducer.
                # Both classes are re-entrant and thread safe.
                GreynirCorrect._parser = edp = ErrorDetectingParser()  # type: ignore[reportIncompatibleVarType]
                GreynirCorrect._reducer = Reducer(edp.grammar)
            return GreynirCorrect._parser

    @property
    def reducer(self) -> Reducer:
        """Return the reducer instance to be used"""
        # Should always retrieve the parser attribute first
        assert GreynirCorrect._reducer is not None
        return GreynirCorrect._reducer

    def annotate(self, sent: Sentence) -> List[Annotation]:
        """Returns a list of annotations for a sentence object, containing
        spelling and grammar annotations of that sentence"""
        ann: List[Annotation] = []
        parsed = sent.deep_tree is not None
        # Create a mapping from token indices to terminal indices.
        # This is necessary because not all tokens are included in
        # the token list that is passed to the parser, and therefore
        # the terminal-token matches can be fewer than the original tokens.
        token_to_terminal: Dict[int, int] = {}
        if parsed:
            token_to_terminal = {
                tnode.index: ix for ix, tnode in enumerate(sent.terminal_nodes) if tnode.index is not None
            }
        grammar = self.parser.grammar
        # First, add token-level annotations and count words that occur in BÍN
        words_in_bin = 0
        words_not_in_bin = 0
        for ix, t in enumerate(sent.tokens):
            if t.kind == TOK.WORD:
                if t.has_meanings:
                    # The word has at least one meaning
                    words_in_bin += 1
                else:
                    # The word has no recognized meaning
                    words_not_in_bin += 1
            elif t.kind == TOK.PERSON:
                # Person names count as recognized words
                words_in_bin += 1
            elif t.kind == TOK.ENTITY:
                # Entity names do not count as recognized words;
                # we count each enclosed word in the entity name
                words_not_in_bin += t.txt.count(" ") + 1
            # Note: these tokens and indices are the original tokens from
            # the submitted text, including ones that are not understood
            # by the parser, such as quotation marks and exotic punctuation
            annotate = False
            if getattr(t, "error_code", "") != "":
                # This is a CorrectToken instance (or a duck typing equivalent)
                assert isinstance(t, CorrectToken)  # Satisfy Mypy
                annotate = True
                if parsed and ix in token_to_terminal:
                    # For the call to suggestion_does_not_match(), we need a
                    # BIN_Token instance, which we obtain in a bit of a hacky
                    # way by creating it on the fly
                    bin_token = BIN_Parser.wrap_token(t, ix)
                    # Obtain the original BIN_Terminal instance from the grammar
                    terminal_index = token_to_terminal[ix]
                    terminal_node = sent.terminal_nodes[terminal_index]
                    original_terminal = terminal_node.original_terminal
                    if original_terminal not in grammar.terminals:
                        # At least one case, finna→Finna, gets the terminal "person_kvk"
                        # which isn't found in grammar.terminals!
                        annotate = False
                        continue
                    assert original_terminal is not None
                    terminal = grammar.terminals[original_terminal]
                    assert isinstance(terminal, VariantHandler)
                    if t.suggestion_does_not_match(terminal, bin_token):
                        # If this token is annotated with a spelling suggestion,
                        # do not add it unless it works grammatically
                        annotate = False
                if annotate:
                    a = Annotation(
                        start=ix,
                        end=ix + t.error_span - 1,
                        code=t.error_code,
                        text=t.error_description,
                        detail=t.error_detail,
                        references=t.error_references,
                        original=t.error_original,
                        suggest=t.error_suggest,
                    )
                    ann.append(a)
            elif t.txt in Abbreviations.DICT:
                # We found an abbreviation and we want to write it out
                # TODO handle TOK.Amount
                annotate = True
                if parsed and ix in token_to_terminal:
                    # For the call to suggestion_does_not_match(), we need a
                    # BIN_Token instance, which we obtain in a bit of a hacky
                    # way by creating it on the fly
                    bin_token = BIN_Parser.wrap_token(t, ix)
                    # Obtain the original BIN_Terminal instance from the grammar
                    terminal_index = token_to_terminal[ix]
                    terminal_node = sent.terminal_nodes[terminal_index]
                    original_terminal = terminal_node.original_terminal
                    if original_terminal not in grammar.terminals:
                        # At least one case, finna→Finna, gets the terminal "person_kvk"
                        # which isn't found in grammar.terminals!
                        annotate = False
                        continue
                    assert original_terminal is not None
                    terminal = grammar.terminals[original_terminal]
                    assert isinstance(terminal, VariantHandler)
                    if t.suggestion_does_not_match(terminal, bin_token):  # type: ignore
                        # If this token is annotated with a spelling suggestion,
                        # do not add it unless it works grammatically
                        annotate = False
                if annotate:
                    a = Annotation(
                        start=ix,
                        end=ix,
                        code="E006",
                        text="Skammstafanir ætti að skrifa út",
                        detail="Best er að skrifa skammstafanir út í formlegu máli.",
                        original=getattr(t, "original", ""),
                        suggest=getattr(t, "txt", ""),
                    )
                    ann.append(a)

            elif "!" in t.txt:
                # Check for exclamation marks in sentence, we wan't to avoid those in professional texts.
                annotate = True
                a = Annotation(
                    start=ix,
                    end=ix,
                    code="E007",
                    text="Setning inniheldur upphrópunarmerki",
                    detail="Best er að forðast upphrópunarmerki í formlegu máli.",
                    original=getattr(t, "original", ""),
                    suggest=getattr(t, "txt", ""),
                )
                ann.append(a)

        # Then, look at the whole sentence
        num_words = words_in_bin + words_not_in_bin
        if num_words > 2 and words_in_bin / num_words < ICELANDIC_RATIO:
            # The sentence contains less than 50% Icelandic
            # words: assume it's in a foreign language and discard the
            # token level annotations
            ann = [
                # E004: The sentence is probably not in Icelandic
                Annotation(
                    start=0,
                    end=len(sent.tokens) - 1,
                    code="E004",
                    text="Málsgreinin er sennilega ekki á íslensku",
                    detail="{0:.0f}% orða í henni finnast ekki í íslenskri orðabók".format(
                        words_not_in_bin / num_words * 100.0
                    ),
                )
            ]
        elif num_words >= 30:
            # The sentence contains 30 words or more, and should be split into shorter
            # sentences to make the text easier to read.
            a = Annotation(
                start=0,
                end=len(sent.tokens) - 1,
                code="E005",
                text="Málsgreinin er í lengra lagi",
                detail="Setningin er {0} orð, betra væri að skipta henni í styttri, auðlæsilegri setningar".format(
                    num_words
                ),
            )
            ann.append(a)

        elif not parsed:
            # If the sentence couldn't be parsed,
            # put an annotation on it as a whole.
            # In this case, we keep the token-level annotations.
            err_index = sent.err_index or 0
            start = max(0, err_index - 1)
            end = min(len(sent.tokens), err_index + 2)
            toktext = correct_spaces(" ".join(t.txt for t in sent.tokens[start:end] if t.txt))
            ann.append(
                # E001: Unable to parse sentence
                Annotation(
                    start=0,
                    end=len(sent.tokens) - 1,
                    code="E001",
                    # Formerly "Málsgreinin fellur ekki að reglum"
                    text="Ekki tókst að þátta setningu; mögulega felst villa í henni",
                    detail="Þáttun brást í kringum {0}. tóka ('{1}')".format(err_index + 1, toktext),
                )
            )
        else:
            # Successfully parsed:
            # Add annotations for error-marked nonterminals from the grammar
            # found in the parse tree
            ErrorFinder(ann, sent).run()
            pm = PatternMatcher(ann, sent)
            # Check whether external tone of voice patterns are given
            if self.settings.tone_of_voice_patterns.PATH:
                file_path = self.settings.tone_of_voice_patterns.PATH
                module_name = os.path.splitext(os.path.basename(file_path))[0]
                # Import the module
                spec: Optional[ModuleSpec] = importlib.util.spec_from_file_location(module_name, file_path)
                if spec is None:
                    raise FileNotFoundError(f"Could not find a spec for module '{module_name}' at '{file_path}'")
                assert isinstance(spec.loader, Loader)
                module: ModuleType = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                # Add the external patterns to the pattern matcher
                module.add_extra_patterns(pm)  # type: ignore # Mypy doesn't know about add_extra_patterns()
            # Run the pattern matcher on the sentence,
            # annotating questionable patterns
            pm.run()
        # Sort the annotations by their start token index,
        # and then by decreasing span length
        ann.sort(key=lambda a: (a.start, -a.end))
        # Eliminate duplicates, i.e. identical annotation
        # codes for identical spans
        i = 1
        while i < len(ann):
            a, prev = ann[i], ann[i - 1]
            if a.code == prev.code and a.start == prev.start and a.end == prev.end:
                # Identical annotation: remove it from the list
                del ann[i]
            else:
                # Check the next pair
                i += 1
        return ann

    def create_sentence(self, job: Job, s: TokenList) -> AnnotatedSentence:
        """Create a fresh sentence object and annotate it
        before returning it to the client"""
        sent = AnnotatedSentence(job, s)
        # Add spelling and grammar annotations to the sentence
        sent.annotations = self.annotate(sent)
        return sent

    def parse_all_token_iter(
        self, tokens: Iterable[Tok], *, progress_func: ProgressFunc = None
    ) -> Iterable[AnnotatedSentence]:
        """Parse all tokens in the given iterable, returning iterable sentences."""
        job = _Job(
            self,
            tokens,
            parse=True,
            progress_func=progress_func,
        )
        # Iterating through the sentences in the job causes
        # them to be parsed and their statistics collected
        for sent in job:
            sent = cast(AnnotatedSentence, sent)
            yield sent

    def parse_all_tokens(self, tokens: Iterable[Tok], *, progress_func: ProgressFunc = None) -> CheckResult:
        """Parse all tokens in the given iterable."""
        job = _Job(
            self,
            tokens,
            parse=True,
            progress_func=progress_func,
        )
        # Iterating through the sentences in the job causes
        # them to be parsed and their statistics collected
        sentences = [cast(AnnotatedSentence, sent) for sent in job]
        return CheckResult(
            sentences=sentences,
            num_sentences=job.num_sentences,
            num_parsed=job.num_parsed,
            num_tokens=job.num_tokens,
            ambiguity=job.ambiguity,
            parse_time=job.parse_time,
        )


def check_single(
    sentence_text: str, rc: Optional[GreynirCorrect] = None, **options: Any
) -> Optional[AnnotatedSentence]:
    """Check and annotate a single sentence, given in plain text"""
    # Returns None if no sentence was parsed
    max_sent_tokens = options.pop("max_sent_tokens", DEFAULT_MAX_SENT_TOKENS)
    if rc is None:
        settings = settings_or_default()
        pipeline = CorrectionPipeline("", settings, **options)
        rc = GreynirCorrect(settings, pipeline, **options)
    return cast(AnnotatedSentence, rc.parse_single(sentence_text, max_sent_tokens=max_sent_tokens))


def check_tokens(
    tokens: Iterable[CorrectToken], rc: Optional[GreynirCorrect] = None, **options: Any
) -> Optional[Sentence]:
    """Check and annotate a single sentence, given as a token list"""
    # Returns None if no sentence was parsed
    max_sent_tokens = options.pop("max_sent_tokens", DEFAULT_MAX_SENT_TOKENS)
    if rc is None:
        settings = settings_or_default()
        pipeline = CorrectionPipeline("", settings, **options)
        rc = GreynirCorrect(settings, pipeline, **options)
    return rc.parse_tokens(tokens, max_sent_tokens=max_sent_tokens)


def check(
    text: str, rc: Optional[GreynirCorrect] = None, **options: Any
) -> Iterable[Paragraph]:
    """Return a generator of checked paragraphs of text,
    each being a generator of checked sentences with
    annotations"""
    split_paragraphs = options.pop("split_paragraphs", False)
    max_sent_tokens = options.pop("max_sent_tokens", DEFAULT_MAX_SENT_TOKENS)
    if rc is None:
        settings = settings_or_default()
        pipeline = CorrectionPipeline("", settings, **options)
        rc = GreynirCorrect(settings, pipeline, **options)
    # This is an asynchronous (on-demand) parse job
    job = rc.submit(
        text,
        parse=True,
        split_paragraphs=split_paragraphs,
        max_sent_tokens=max_sent_tokens,
    )
    yield from job.paragraphs()


def check_with_stats(
    text: str,
    *,
    settings: Optional[Settings] = None,
    split_paragraphs: bool = False,
    progress_func: ProgressFunc = None,
    **options: Any,
) -> CheckResult:
    """Return a dict containing parsed paragraphs as well as statistics,
    using the given correction/parser class. This is a low-level
    function; normally check_with_stats() should be used."""
    settings = settings_or_default(settings)
    split_paragraphs = options.pop("split_paragraphs", False)
    max_sent_tokens = options.pop("max_sent_tokens", DEFAULT_MAX_SENT_TOKENS)
    pipeline = CorrectionPipeline("", settings, **options)
    rc = GreynirCorrect(settings, pipeline, **options)
    # This is an asynchronous (on-demand) parse job
    job = rc.submit(
        text,
        parse=True,
        split_paragraphs=split_paragraphs,
        progress_func=progress_func,
        max_sent_tokens=max_sent_tokens,
    )
    # Enumerating through the job's paragraphs and sentences causes them
    # to be parsed and their statistics collected
    sentences = [cast(AnnotatedSentence, sent) for pg in job.paragraphs() for sent in pg]
    return CheckResult(
        sentences=sentences,
        num_tokens=job.num_tokens,
        num_sentences=job.num_sentences,
        num_parsed=job.num_parsed,
        ambiguity=job.ambiguity,
        parse_time=job.parse_time,
    )
